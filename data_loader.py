# -*- coding: utf-8 -*-
"""data_loader.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U932589_d9VOWHzYLmQRx-tNCuhExgwl
"""

import os
import json
import random
from datasets import load_dataset
from transformers import AutoTokenizer

# Config
HF_DATASET = "lavita/AlpaCare-MedInstruct-52k"
BASE_MODEL = "stabilityai/stablelm-tuned-alpha-3b"  # change if using another
OUTPUT_DIR = "processed_data"
SPLIT_RATIOS = (0.90, 0.05, 0.05)  # train / val / test
RANDOM_SEED = 42
MAX_EXAMPLES = None  # set an int (e.g., 2000) for quick Colab runs

os.makedirs(OUTPUT_DIR, exist_ok=True)


def basic_clean(text: str) -> str:
    """Normalize whitespace and strip."""
    if text is None:
        return ""
    return " ".join(text.strip().split())


def normalize_example(ex: dict) -> dict:
    """Map dataset fields and filter unsafe samples."""
    inst = ex.get("instruction") or ex.get("prompt") or ""
    resp = ex.get("response") or ex.get("output") or ""

    inst = basic_clean(inst)
    resp = basic_clean(resp)

    # Filter: remove samples that include diagnosis / prescriptions
    forbidden = ["diagnos", "prescrib", "dosage", "dose", "give medicine"]
    combined = (inst + " " + resp).lower()
    if any(k in combined for k in forbidden):
        return None

    return {"instruction": inst, "response": resp}


def load_and_clean():
    """Load dataset and return list of cleaned records."""
    ds = load_dataset(HF_DATASET)
    records = []
    for split in ds:
        for ex in ds[split]:
            norm = normalize_example(ex)
            if norm:
                records.append(norm)
            if MAX_EXAMPLES and len(records) >= MAX_EXAMPLES:
                break
        if MAX_EXAMPLES and len(records) >= MAX_EXAMPLES:
            break

    random.Random(RANDOM_SEED).shuffle(records)
    return records


def split_and_save(records):
    """Split into train/val/test and save as JSONL."""
    n = len(records)
    n_train = int(n * SPLIT_RATIOS[0])
    n_val = int(n * SPLIT_RATIOS[1])

    train = records[:n_train]
    val = records[n_train:n_train+n_val]
    test = records[n_train+n_val:]

    for name, arr in [("train", train), ("validation", val), ("test", test)]:
        path = os.path.join(OUTPUT_DIR, f"{name}.jsonl")
        with open(path, "w", encoding="utf-8") as f:
            for r in arr:
                f.write(json.dumps(r, ensure_ascii=False) + "\n")
        print(f"âœ… Saved {len(arr)} examples to {path}")


def tokenize_and_save(tokenizer_name=BASE_MODEL, max_length=512):
    """Optional: tokenize dataset and save tokenized JSONL."""
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)
    tok_dir = os.path.join(OUTPUT_DIR, "tokenized")
    os.makedirs(tok_dir, exist_ok=True)

    for split in ["train", "validation", "test"]:
        in_path = os.path.join(OUTPUT_DIR, f"{split}.jsonl")
        out_path = os.path.join(tok_dir, f"{split}_tok.jsonl")

        with open(in_path, "r", encoding="utf-8") as fin, open(out_path, "w", encoding="utf-8") as fout:
            for line in fin:
                obj = json.loads(line)
                prompt = f"Instruction: {obj['instruction']}\n\nResponse:"
                enc = tokenizer(prompt, truncation=True, max_length=max_length, padding=False)
                fout.write(json.dumps({
                    "input_ids": enc["input_ids"],
                    "attention_mask": enc["attention_mask"],
                    "response": obj["response"]
                }) + "\n")
        print(f"ğŸ”‘ Tokenized {split} -> {out_path}")


if __name__ == "__main__":
    recs = load_and_clean()
    split_and_save(recs)