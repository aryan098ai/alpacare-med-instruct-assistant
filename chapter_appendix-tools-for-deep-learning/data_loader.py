{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "data_loader.py\n",
        "---------------\n",
        "Loads the lavita/AlpaCare-MedInstruct-52k dataset from Hugging Face,\n",
        "applies simple cleaning and filtering, splits into train/val/test,\n",
        "and saves as JSONL files. Optionally tokenizes for LM training.\n",
        "\n",
        "Usage:\n",
        "    python data_loader.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Config\n",
        "HF_DATASET = \"lavita/AlpaCare-MedInstruct-52k\"\n",
        "BASE_MODEL = \"stabilityai/stablelm-tuned-alpha-3b\"  # change if using another\n",
        "OUTPUT_DIR = \"processed_data\"\n",
        "SPLIT_RATIOS = (0.90, 0.05, 0.05)  # train / val / test\n",
        "RANDOM_SEED = 42\n",
        "MAX_EXAMPLES = None  # set an int (e.g., 2000) for quick Colab runs\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def basic_clean(text: str) -> str:\n",
        "    \"\"\"Normalize whitespace and strip.\"\"\"\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "    return \" \".join(text.strip().split())\n",
        "\n",
        "\n",
        "def normalize_example(ex: dict) -> dict:\n",
        "    \"\"\"Map dataset fields and filter unsafe samples.\"\"\"\n",
        "    inst = ex.get(\"instruction\") or ex.get(\"prompt\") or \"\"\n",
        "    resp = ex.get(\"response\") or ex.get(\"output\") or \"\"\n",
        "\n",
        "    inst = basic_clean(inst)\n",
        "    resp = basic_clean(resp)\n",
        "\n",
        "    # Filter: remove samples that include diagnosis / prescriptions\n",
        "    forbidden = [\"diagnos\", \"prescrib\", \"dosage\", \"dose\", \"give medicine\"]\n",
        "    combined = (inst + \" \" + resp).lower()\n",
        "    if any(k in combined for k in forbidden):\n",
        "        return None\n",
        "\n",
        "    return {\"instruction\": inst, \"response\": resp}\n",
        "\n",
        "\n",
        "def load_and_clean():\n",
        "    \"\"\"Load dataset and return list of cleaned records.\"\"\"\n",
        "    ds = load_dataset(HF_DATASET)\n",
        "    records = []\n",
        "    for split in ds:\n",
        "        for ex in ds[split]:\n",
        "            norm = normalize_example(ex)\n",
        "            if norm:\n",
        "                records.append(norm)\n",
        "            if MAX_EXAMPLES and len(records) >= MAX_EXAMPLES:\n",
        "                break\n",
        "        if MAX_EXAMPLES and len(records) >= MAX_EXAMPLES:\n",
        "            break\n",
        "\n",
        "    random.Random(RANDOM_SEED).shuffle(records)\n",
        "    return records\n",
        "\n",
        "\n",
        "def split_and_save(records):\n",
        "    \"\"\"Split into train/val/test and save as JSONL.\"\"\"\n",
        "    n = len(records)\n",
        "    n_train = int(n * SPLIT_RATIOS[0])\n",
        "    n_val = int(n * SPLIT_RATIOS[1])\n",
        "\n",
        "    train = records[:n_train]\n",
        "    val = records[n_train:n_train+n_val]\n",
        "    test = records[n_train+n_val:]\n",
        "\n",
        "    for name, arr in [(\"train\", train), (\"validation\", val), (\"test\", test)]:\n",
        "        path = os.path.join(OUTPUT_DIR, f\"{name}.jsonl\")\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            for r in arr:\n",
        "                f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "        print(f\"âœ… Saved {len(arr)} examples to {path}\")\n",
        "\n",
        "\n",
        "def tokenize_and_save(tokenizer_name=BASE_MODEL, max_length=512):\n",
        "    \"\"\"Optional: tokenize dataset and save tokenized JSONL.\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
        "    tok_dir = os.path.join(OUTPUT_DIR, \"tokenized\")\n",
        "    os.makedirs(tok_dir, exist_ok=True)\n",
        "\n",
        "    for split in [\"train\", \"validation\", \"test\"]:\n",
        "        in_path = os.path.join(OUTPUT_DIR, f\"{split}.jsonl\")\n",
        "        out_path = os.path.join(tok_dir, f\"{split}_tok.jsonl\")\n",
        "\n",
        "        with open(in_path, \"r\", encoding=\"utf-8\") as fin, open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "            for line in fin:\n",
        "                obj = json.loads(line)\n",
        "                prompt = f\"Instruction: {obj['instruction']}\\n\\nResponse:\"\n",
        "                enc = tokenizer(prompt, truncation=True, max_length=max_length, padding=False)\n",
        "                fout.write(json.dumps({\n",
        "                    \"input_ids\": enc[\"input_ids\"],\n",
        "                    \"attention_mask\": enc[\"attention_mask\"],\n",
        "                    \"response\": obj[\"response\"]\n",
        "                }) + \"\\n\")\n",
        "        print(f\"ðŸ”‘ Tokenized {split} -> {out_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    recs = load_and_clean()\n",
        "    split_and_save(recs)\n",
        "    # Uncomment to also tokenize:\n",
        "    # tokenize_and_save()\n"
      ],
      "metadata": {
        "id": "BXEpNP7Up0bE"
      },
      "id": "BXEpNP7Up0bE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}