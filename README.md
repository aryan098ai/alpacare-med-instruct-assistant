# AlpaCare â€” Medical Instruction Assistant (LoRA / PEFT)

**Goal:** Fine-tune a pre-trained LLM (<7B) on the `lavita/AlpaCare-MedInstruct-52k` dataset to build a safe, **non-diagnostic** medical instruction assistant using LoRA/PEFT on Google Colab.  
**Note:** This assistant must never provide diagnosis, prescriptions, or dosage. Every output includes the disclaimer:

> *"This is educational only â€” consult a qualified clinician."*

---


## ğŸ“‚ Repository Structure

ğŸ“„ [README.md](README.md)  
ğŸ“„ [requirements.txt](requirements.txt)  
ğŸ“„ [data_loader.py](data_loader.py)  
ğŸ“„ [REPORT.pdf](REPORT.pdf)  

ğŸ“‚ [human_eval](human_eval)  
â”œâ”€â”€ ğŸ“„ [human_eval_rubric.csv](human_eval/human_eval_rubric.csv)  
â””â”€â”€ ğŸ“„ [human_eval_responses.csv](human_eval/human_eval_responses.csv)  

ğŸ“‚ [adapters](adapters)  
â””â”€â”€ [alpacare_lora_adapter.zip](alphacare_lora_adapter.zip) 

ğŸ“‚ [notebooks](notebooks)  
â”œâ”€â”€ ğŸ“„ [colab-finetune.ipynb](notebooks/colab-finetune.ipynb)  
â””â”€â”€ ğŸ“„ [inference_demo.ipynb](notebooks/inference_demo.ipynb)  

ğŸ“„ [LICENSE](LICENSE)  



---

## ğŸš€ Quick start (Colab)
1. Open `notebooks/colab-finetune.ipynb` in Google Colab (GPU runtime).
2. Run all cells. If GPU memory is low, switch to the â€œsubset modeâ€ (small data slice).
3. Training saves LoRA adapter to `/content/drive/MyDrive/alpacare_adapter/` or `adapters/`.
4. Use `notebooks/inference_demo.ipynb` to load base model + adapter and test prompts (disclaimer included).

---

## ğŸ§  Model choice & license
- Suggested base models (pick one under 7B):
  - `stabilityai/stablelm-tuned-alpha-3b` (â‰ˆ3B)  
  - `EleutherAI/gpt-neox-3.6b` (â‰ˆ3.6B)  
- Confirm license terms before training.

---

## ğŸ“Š Dataset
- Source: `lavita/AlpaCare-MedInstruct-52k` (Hugging Face).
- Splits: 90% train, 5% val, 5% test.
- Subset used in demo: small slice for Colab GPU constraints.
- Preprocessing & cleaning in `data_loader.py`.

---

## ğŸ› ï¸ Dependencies
See `requirements.txt`.  
Main libs: `transformers`, `datasets`, `accelerate`, `peft`, `bitsandbytes`, `torch`.

---

## ğŸ“¦ Deliverables
- LoRA adapter (saved via `PeftModel.save_pretrained()`).
- `REPORT.pdf` (â‰¤6 pages: dataset, approach, eval, safety).
- Human eval CSVs (â‰¥30 reviews).
- Notebooks for training & inference.

---

## âš ï¸ Safety
- Filtered training set: removed diagnosis/prescription samples.
- All outputs include disclaimer.
- Human evaluation by medically literate reviewers.
- Not for clinical use.

---

## ğŸ‘¤ Maintainer
- Aryan Shukla / aryan098ai / shuklaaryan070@gmail.com
